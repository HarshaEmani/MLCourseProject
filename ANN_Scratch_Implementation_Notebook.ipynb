{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Artificial Neural Network Implementation from Scratch\n",
    "\n",
    "The below implementation for Artificial Neural Network is a Brute Force implementation for binary classification of data into 0 or 1 based on the features of count n.This algorithm would work well for binary classification, but not multi-class classification because a lot of code for loss function and activation is hard coded to suitthe task.\n",
    "Task: Implementation of an Artificial Neural Network for programming project assignment for Machine Learning and Data Mining course by Dr. Huajie Zhang atUniversity of New Brunswick.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Dataset\n",
    "\n",
    "This is the dataset that was provided with the assignment:\n",
    "https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Environment\n",
    "\n",
    "Programming Language: Python 3.12.0\n",
    "\n",
    "IDE: PyCharm Professional 23.1\n",
    "\n",
    "Libraries Used: Numpy, Pandas, UCIMLRepo, SKLearn, Math, Matplotlib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "The below code is to import different libraries and modules to build an artificial neural network. Since the ANN is being built from scratch, too many libraries are notneeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:04:42.987984500Z",
     "start_time": "2024-12-06T19:04:42.756834900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Standard library for data science\n",
    "import pandas as pd # Standard library for data science\n",
    "from ucimlrepo import fetch_ucirepo # To fetch the breast cancer data\n",
    "\n",
    "# For standardizing features and encoding categorical labels to 0 or 1\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Split the input/output data to train and test data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To check the model for accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "# For any standard math operations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To plot any graphs\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic\n",
    "\n",
    "Below module is the crux of this whole ANN implementation. This is where all the magic is happening.\n",
    "\n",
    "Weights, Bias, Activation, Forward Propagation, Cost, Loss, Derivatives, Learning Rate, Backward Propagation. Repeat (IYKYK).\n",
    "\n",
    "In this code, the activation function is sigmoid and loss function is mean squared error.\n",
    "\n",
    "Why: Multiple experiements with relu, softmax etc. didn't achieve good results. Same for loss functions - categorical cross entropy and sparse categorical crossentropy were used for initial experiments. After a lot of research, trial and error, sigmoid and MSE work well for basic binary classification task with an ANNimplemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Neural Network class which is used to build, train and predict values for the model.\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, layer_sizes, alpha):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.alpha = alpha\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1  # Number of layers (excluding input layer)\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.W[str(l)] = np.random.randn(\n",
    "                layer_sizes[l], layer_sizes[l - 1]\n",
    "            ) * math.sqrt(2 / layer_sizes[l - 1])\n",
    "            self.b[str(l)] = np.zeros((layer_sizes[l], 1))\n",
    "\n",
    "        # Cache for intermediate values in forward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(arr):\n",
    "        # Sigmoid activation function.\n",
    "        return 1 / (1 + np.exp(-arr))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(arr):\n",
    "        # Derivative of the sigmoid function.\n",
    "        return arr * (1 - arr)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return np.heaviside(x, 0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward propagation through all layers.\n",
    "        A = X\n",
    "        self.cache[\"A0\"] = A  # Store the input layer activation\n",
    "        for l in range(1, self.L + 1):\n",
    "            Z = self.W[str(l)] @ A + self.b[str(l)]\n",
    "            A = self.sigmoid(Z)\n",
    "            self.cache[f\"A{l}\"] = A  # Cache activation\n",
    "        return A\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def compute_cost(self, y_hat, y):\n",
    "        # Calculate mean squared error as cost.\n",
    "        m = y.shape[1]\n",
    "        return np.sum((y_hat - y) ** 2) / (2 * m)\n",
    "\n",
    "    def backward(self, y_hat):\n",
    "        # Backward propagation to compute gradients for weights and biases.\n",
    "        m = self.X.shape[1]\n",
    "        grads = {}\n",
    "        dA = (\n",
    "            y_hat - self.y\n",
    "        ) / m  # Initial gradient based on cost function (mean squared error)\n",
    "\n",
    "        for l in reversed(range(1, self.L + 1)):\n",
    "            dZ = dA * self.sigmoid_derivative(self.cache[f\"A{l}\"])  # Element-wise gradient\n",
    "            grads[f\"dW{l}\"] = dZ @ self.cache[f\"A{l - 1}\"].T\n",
    "            grads[f\"db{l}\"] = np.sum(dZ, axis=1, keepdims=True)\n",
    "            if l > 1:  # Calculate dA for the previous layer if not at the input layer\n",
    "                dA = self.W[str(l)].T @ dZ\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "        # Update weights and biases using gradients.\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.W[str(l)] -= self.alpha * grads[f\"dW{l}\"]\n",
    "            self.b[str(l)] -= self.alpha * grads[f\"db{l}\"]\n",
    "\n",
    "    def train(self, epochs=1000):\n",
    "        # Train the network for a set number of epochs.\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.forward(self.X)\n",
    "            cost = self.compute_cost(y_hat, self.y)\n",
    "            grads = self.backward(y_hat)\n",
    "            self.update_parameters(grads)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict output for given input data.\n",
    "        return self.forward(X)\n",
    "\n",
    "    def accuracy(self, y_hat, y):\n",
    "        # Compute accuracy as percentage of correct predictions (rounded).\n",
    "        predictions = np.round(y_hat)  # Rounding since outputs are continuous\n",
    "        accuracy = np.mean(predictions == y) * 100\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching and Preprocessing\n",
    "\n",
    "This model was specifically developed to classify the diagnostic data of images for breast cancer cells to classify as 'B' or 'M'. More details about what they meancan be found in the dataset page. There wasn't much preprocessing needed for the data as there were no missing data or outliers as mentioned in the datasetpage. This helped in just focusing on standardization and label encoding of data and preparing data for training the ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 426), (1, 426), (30, 143), (1, 143))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "\n",
    "# To scale and standardize X\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# To encode the labels of y\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Scaled\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshaped to a 1D array to suppress a warning from LabelEncoder\n",
    "y = np.array(y).reshape(-1)\n",
    "\n",
    "# Encoded\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Reshaped back to a 2D array after encoding\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Split X to X_train, X_test and y to y_train and y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, shuffle=True)\n",
    "\n",
    "X_train = X_train.T  # Transpose to shape (30, 426)\n",
    "y_train = y_train.T  # Transpose to shape (1, 426)\n",
    "X_test = X_test.T  # Transpose to shape (30, 143)\n",
    "y_test = y_test.T  # Transpose to shape (1, 143)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "layer_sizes specifies the number of layers and number of neurons each layer uses. The length of layer_sizes is the total number of layers we use and the size ofeach layer is the number of neurons in that layer.\n",
    "\n",
    "For example: layer_sizes=[30, 100, 10, 1] means that there are a total of 4 layers, including 1 input layer, 2 hidden layers and 1 output layer.\n",
    "\n",
    "30 indicates that there are 30 neurons in the input layer, 100 in the first hidden layer and 10 in the second. Only one neuron in output layer for predicting the labelof y.\n",
    "\n",
    "The model was trained for different epochs to see how well it performs as the number of iterations increases. After a lot of experimentation, 0.01 learning rateworked best for most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.10748675356041129\n",
      "Epoch 0, Cost: 0.13500955681811105\n",
      "Epoch 0, Cost: 0.16520182945418146\n",
      "Epoch 100, Cost: 0.13164190711656504\n",
      "Epoch 200, Cost: 0.1136566206853964\n",
      "Epoch 300, Cost: 0.10449604037406054\n",
      "Epoch 400, Cost: 0.09908297619631976\n",
      "Epoch 500, Cost: 0.09518814194142751\n",
      "Epoch 600, Cost: 0.09192551531552982\n",
      "Epoch 700, Cost: 0.08894355618921909\n",
      "Epoch 800, Cost: 0.08610219986037088\n",
      "Epoch 900, Cost: 0.08334607427253138\n",
      "Epoch 0, Cost: 0.12655828347740633\n",
      "Epoch 100, Cost: 0.12313207979330536\n",
      "Epoch 200, Cost: 0.11980470256009032\n",
      "Epoch 300, Cost: 0.11659264872490793\n",
      "Epoch 400, Cost: 0.11349197488791535\n",
      "Epoch 500, Cost: 0.11048709468981692\n",
      "Epoch 600, Cost: 0.10756106712812577\n",
      "Epoch 700, Cost: 0.1047026862587927\n",
      "Epoch 800, Cost: 0.10190824867340849\n",
      "Epoch 900, Cost: 0.09917924277985331\n",
      "Epoch 1000, Cost: 0.09651887308077226\n",
      "Epoch 1100, Cost: 0.09392960354606815\n",
      "Epoch 1200, Cost: 0.09141223881073576\n",
      "Epoch 1300, Cost: 0.08896603605214659\n",
      "Epoch 1400, Cost: 0.08658921526047243\n",
      "Epoch 1500, Cost: 0.08427949178755734\n",
      "Epoch 1600, Cost: 0.08203448888707104\n",
      "Epoch 1700, Cost: 0.07985200639818692\n",
      "Epoch 1800, Cost: 0.07773016477069306\n",
      "Epoch 1900, Cost: 0.07566745476047897\n",
      "Epoch 2000, Cost: 0.07366272293327836\n",
      "Epoch 2100, Cost: 0.07171511879087349\n",
      "Epoch 2200, Cost: 0.0698240235264828\n",
      "Epoch 2300, Cost: 0.06798897452410106\n",
      "Epoch 2400, Cost: 0.0662095945746489\n",
      "Epoch 2500, Cost: 0.06448553076687634\n",
      "Epoch 2600, Cost: 0.06281640515658728\n",
      "Epoch 2700, Cost: 0.061201777461731346\n",
      "Epoch 2800, Cost: 0.059641118939551666\n",
      "Epoch 2900, Cost: 0.05813379604697922\n",
      "Epoch 3000, Cost: 0.05667906227911918\n",
      "Epoch 3100, Cost: 0.05527605658421578\n",
      "Epoch 3200, Cost: 0.05392380687201378\n",
      "Epoch 3300, Cost: 0.052621237305124004\n",
      "Epoch 3400, Cost: 0.0513671782531634\n",
      "Epoch 3500, Cost: 0.05016037797639942\n",
      "Epoch 3600, Cost: 0.04899951527872625\n",
      "Epoch 3700, Cost: 0.04788321252435647\n",
      "Epoch 3800, Cost: 0.04681004854730247\n",
      "Epoch 3900, Cost: 0.04577857109802249\n",
      "Epoch 4000, Cost: 0.044787308568847384\n",
      "Epoch 4100, Cost: 0.04383478082068234\n",
      "Epoch 4200, Cost: 0.04291950899978109\n",
      "Epoch 4300, Cost: 0.04204002428686404\n",
      "Epoch 4400, Cost: 0.041194875563155195\n",
      "Epoch 4500, Cost: 0.04038263601057993\n",
      "Epoch 4600, Cost: 0.039601908687813235\n",
      "Epoch 4700, Cost: 0.03885133114138796\n",
      "Epoch 4800, Cost: 0.03812957912283337\n",
      "Epoch 4900, Cost: 0.037435369489869745\n",
      "Epoch 0, Cost: 0.16842987771201606\n",
      "Epoch 100, Cost: 0.13931682626769068\n",
      "Epoch 200, Cost: 0.12140727257969167\n",
      "Epoch 300, Cost: 0.11129096306119472\n",
      "Epoch 400, Cost: 0.10476968929760862\n",
      "Epoch 500, Cost: 0.09973500325598098\n",
      "Epoch 600, Cost: 0.09535289908380572\n",
      "Epoch 700, Cost: 0.09131712161250709\n",
      "Epoch 800, Cost: 0.08751901862527828\n",
      "Epoch 900, Cost: 0.08392061607709805\n",
      "Epoch 1000, Cost: 0.08050804354238468\n",
      "Epoch 1100, Cost: 0.07727508055907926\n",
      "Epoch 1200, Cost: 0.07421751519463089\n",
      "Epoch 1300, Cost: 0.07133119692163585\n",
      "Epoch 1400, Cost: 0.0686113355547606\n",
      "Epoch 1500, Cost: 0.06605227097740468\n",
      "Epoch 1600, Cost: 0.06364747190699203\n",
      "Epoch 1700, Cost: 0.061389666809208994\n",
      "Epoch 1800, Cost: 0.05927104533685874\n",
      "Epoch 1900, Cost: 0.05728348289997516\n",
      "Epoch 2000, Cost: 0.05541875457140698\n",
      "Epoch 2100, Cost: 0.053668718252723645\n",
      "Epoch 2200, Cost: 0.052025458527799016\n",
      "Epoch 2300, Cost: 0.050481390683010365\n",
      "Epoch 2400, Cost: 0.049029329061443486\n",
      "Epoch 2500, Cost: 0.047662526007125165\n",
      "Epoch 2600, Cost: 0.046374688041504804\n",
      "Epoch 2700, Cost: 0.04515997535309131\n",
      "Epoch 2800, Cost: 0.04401298970327612\n",
      "Epoch 2900, Cost: 0.04292875478230747\n",
      "Epoch 3000, Cost: 0.04190269206165185\n",
      "Epoch 3100, Cost: 0.04093059435583665\n",
      "Epoch 3200, Cost: 0.0400085986441766\n",
      "Epoch 3300, Cost: 0.0391331591973791\n",
      "Epoch 3400, Cost: 0.03830102168099173\n",
      "Epoch 3500, Cost: 0.037509198639834596\n",
      "Epoch 3600, Cost: 0.036754946580123085\n",
      "Epoch 3700, Cost: 0.0360357447381832\n",
      "Epoch 3800, Cost: 0.03534927554007583\n",
      "Epoch 3900, Cost: 0.03469340670256708\n",
      "Epoch 4000, Cost: 0.03406617489342207\n",
      "Epoch 4100, Cost: 0.033465770851212204\n",
      "Epoch 4200, Cost: 0.03289052585688562\n",
      "Epoch 4300, Cost: 0.03233889944785876\n",
      "Epoch 4400, Cost: 0.03180946826797873\n",
      "Epoch 4500, Cost: 0.03130091595176031\n",
      "Epoch 4600, Cost: 0.030812023947719293\n",
      "Epoch 4700, Cost: 0.030341663192675714\n",
      "Epoch 4800, Cost: 0.029888786556111904\n",
      "Epoch 4900, Cost: 0.029452421980739028\n",
      "Epoch 5000, Cost: 0.02903166625217405\n",
      "Epoch 5100, Cost: 0.0286256793369531\n",
      "Epoch 5200, Cost: 0.028233679233960536\n",
      "Epoch 5300, Cost: 0.027854937289722426\n",
      "Epoch 5400, Cost: 0.027488773932905157\n",
      "Epoch 5500, Cost: 0.027134554787797645\n",
      "Epoch 5600, Cost: 0.02679168713056588\n",
      "Epoch 5700, Cost: 0.026459616655684394\n",
      "Epoch 5800, Cost: 0.026137824523203394\n",
      "Epoch 5900, Cost: 0.025825824660435096\n",
      "Epoch 6000, Cost: 0.025523161294269566\n",
      "Epoch 6100, Cost: 0.02522940669268889\n",
      "Epoch 6200, Cost: 0.024944159096163963\n",
      "Epoch 6300, Cost: 0.024667040821517657\n",
      "Epoch 6400, Cost: 0.024397696522542082\n",
      "Epoch 6500, Cost: 0.024135791593187253\n",
      "Epoch 6600, Cost: 0.023881010700512153\n",
      "Epoch 6700, Cost: 0.023633056435822412\n",
      "Epoch 6800, Cost: 0.023391648073527817\n",
      "Epoch 6900, Cost: 0.023156520428248572\n",
      "Epoch 7000, Cost: 0.022927422801596205\n",
      "Epoch 7100, Cost: 0.022704118010860957\n",
      "Epoch 7200, Cost: 0.022486381492563674\n",
      "Epoch 7300, Cost: 0.02227400047448454\n",
      "Epoch 7400, Cost: 0.02206677321037022\n",
      "Epoch 7500, Cost: 0.021864508272052906\n",
      "Epoch 7600, Cost: 0.021667023894194427\n",
      "Epoch 7700, Cost: 0.021474147367301792\n",
      "Epoch 7800, Cost: 0.021285714475052064\n",
      "Epoch 7900, Cost: 0.021101568972317915\n",
      "Epoch 8000, Cost: 0.020921562100605744\n",
      "Epoch 8100, Cost: 0.020745552137907422\n",
      "Epoch 8200, Cost: 0.020573403980229162\n",
      "Epoch 8300, Cost: 0.020404988752298982\n",
      "Epoch 8400, Cost: 0.020240183445169244\n",
      "Epoch 8500, Cost: 0.020078870578626895\n",
      "Epoch 8600, Cost: 0.01992093788650105\n",
      "Epoch 8700, Cost: 0.019766278023119106\n",
      "Epoch 8800, Cost: 0.019614788289309185\n",
      "Epoch 8900, Cost: 0.019466370376479872\n",
      "Epoch 9000, Cost: 0.01932093012742969\n",
      "Epoch 9100, Cost: 0.01917837731264924\n",
      "Epoch 9200, Cost: 0.019038625420979755\n",
      "Epoch 9300, Cost: 0.018901591463583348\n",
      "Epoch 9400, Cost: 0.018767195790264445\n",
      "Epoch 9500, Cost: 0.018635361917258002\n",
      "Epoch 9600, Cost: 0.018506016365670338\n",
      "Epoch 9700, Cost: 0.018379088509822006\n",
      "Epoch 9800, Cost: 0.01825451043480085\n",
      "Epoch 9900, Cost: 0.018132216802586677\n"
     ]
    }
   ],
   "source": [
    "# To store multiple models\n",
    "models = []\n",
    "\n",
    "# Testing with multiple epoch counts to evaluate the best epoch count\n",
    "epochs=[10, 100, 1000, 5000, 10000]\n",
    "\n",
    "# Iterating for every epoch count, and initializing a model for each\n",
    "for epoch in epochs:\n",
    "    # Model initialization\n",
    "    model = NeuralNet(X_train, y_train, layer_sizes=[30, 100, 10, 1], alpha=0.01)\n",
    "    \n",
    "    # Train the model with epoch count\n",
    "    model.train(epochs=epoch)\n",
    "    models.append(model)\n",
    "\n",
    "# Predict values for every epoch\n",
    "y_pred_array = [model.predict(X_test) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Metrics\n",
    "\n",
    "Metrics for every epoch were calculated and combined the flattened version of y_test and y_pred to view random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.04195804195804%\n",
      "Test results: \n",
      "     Actuals  Predictions\n",
      "40         0          0.0\n",
      "109        0          0.0\n",
      "113        0          0.0\n",
      "126        0          0.0\n",
      "95         1          0.0\n",
      "42         0          0.0\n",
      "135        1          0.0\n",
      "79         1          0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics and results comparison for 10 epochs\n",
    "\n",
    "accuracy_10 = model.accuracy(y_pred_array[0], y_test)\n",
    "\n",
    "results_10_epochs = pd.DataFrame(\n",
    "    {\"Actuals\": y_test.flatten(), \"Predictions\": np.round(y_pred_array[0].flatten())}\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_10}%\")\n",
    "print(f\"Test results: \\n{results_10_epochs.sample(8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.04195804195804%\n",
      "Test results: \n",
      "     Actuals  Predictions\n",
      "129        1          0.0\n",
      "72         0          0.0\n",
      "140        1          0.0\n",
      "8          0          0.0\n",
      "5          0          0.0\n",
      "39         0          0.0\n",
      "18         0          0.0\n",
      "91         1          0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics and results comparison for 100 epochs\n",
    "\n",
    "accuracy_100 = model.accuracy(y_pred_array[1], y_test)\n",
    "\n",
    "results_100_epochs = pd.DataFrame(\n",
    "    {\"Actuals\": y_test.flatten(), \"Predictions\": np.round(y_pred_array[1].flatten())}\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_100}%\")\n",
    "print(f\"Test results: \\n{results_100_epochs.sample(8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.51748251748252%\n",
      "Test results: \n",
      "     Actuals  Predictions\n",
      "7          0          0.0\n",
      "32         1          1.0\n",
      "69         0          0.0\n",
      "84         1          1.0\n",
      "118        1          0.0\n",
      "65         0          0.0\n",
      "134        0          0.0\n",
      "29         0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics and results comparison for 1000 epochs\n",
    "\n",
    "accuracy_1000 = model.accuracy(y_pred_array[2], y_test)\n",
    "\n",
    "results_1000_epochs = pd.DataFrame(\n",
    "    {\"Actuals\": y_test.flatten(), \"Predictions\": np.round(y_pred_array[2].flatten())}\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_1000}%\")\n",
    "print(f\"Test results: \\n{results_1000_epochs.sample(8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.4055944055944%\n",
      "Test results: \n",
      "     Actuals  Predictions\n",
      "136        0          0.0\n",
      "55         0          0.0\n",
      "10         0          0.0\n",
      "76         1          0.0\n",
      "110        0          0.0\n",
      "14         1          1.0\n",
      "82         0          0.0\n",
      "132        0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics and results comparison for 5000 epochs\n",
    "\n",
    "accuracy_5000 = model.accuracy(y_pred_array[3], y_test)\n",
    "\n",
    "results_5000_epochs = pd.DataFrame(\n",
    "    {\"Actuals\": y_test.flatten(), \"Predictions\": np.round(y_pred_array[3].flatten())}\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_5000}%\")\n",
    "print(f\"Test results: \\n{results_5000_epochs.sample(8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.5034965034965%\n",
      "Test results: \n",
      "    Actuals  Predictions\n",
      "40        0          0.0\n",
      "23        1          1.0\n",
      "64        0          0.0\n",
      "71        1          1.0\n",
      "82        0          0.0\n",
      "79        1          1.0\n",
      "84        1          1.0\n",
      "7         0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics and results comparison for 10000 epochs\n",
    "\n",
    "accuracy_10000 = model.accuracy(y_pred_array[4], y_test)\n",
    "\n",
    "results_10000_epochs = pd.DataFrame(\n",
    "    {\"Actuals\": y_test.flatten(), \"Predictions\": np.round(y_pred_array[4].flatten())}\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_10000}%\")\n",
    "print(f\"Test results: \\n{results_10000_epochs.sample(8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n",
    "\n",
    "A visual representation of how well the accuracy increases with the increase in number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdLklEQVR4nO3dfZBV5X3A8d/ytovCLi+WvWyymE1LgqJRA+lmo8k0cceNJalUplGHNtTYkDZoi1iNzATIUBMUWkNJEKKTgs7EpPKHpuaFDLMqjnFddYOJRkq0oYEG7zIt2b1IZUX29I9O7uQKRtS7e59LPp+ZM+M959mzzznHq985e19qsizLAgAgISMqPQEAgFcTKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRnVKUn8GYMDg7Gvn37Yvz48VFTU1Pp6QAAJyDLsjh48GA0NTXFiBG//R5JVQbKvn37orm5udLTAADehL1798bb3/723zrmDQfKww8/HGvWrImenp544YUX4t577425c+cWt2dZFitWrIg77rgj+vr64vzzz48NGzbE9OnTi2MOHDgQ11xzTdx///0xYsSImDdvXvzzP/9zjBs37oTmMH78+OIB1tfXv9FDAAAqoFAoRHNzc/H/47/NGw6UQ4cOxTnnnBOf+tSn4tJLLz1m++rVq2PdunVx5513RktLSyxbtiw6Ojri2Wefjbq6uoiImD9/frzwwguxbdu2OHLkSFx55ZWxcOHCuPvuu09oDr/+s059fb1AAYAqcyIvz6h5K18WWFNTU3IHJcuyaGpqiuuuuy7+/u//PiIi+vv7o7GxMTZv3hyXX3557Ny5M84888x44oknYvbs2RERsXXr1vjjP/7j+K//+q9oamp63d9bKBSioaEh+vv7BQoAVIk38v/vsr6LZ/fu3ZHP56O9vb24rqGhIVpbW6OrqysiIrq6umLChAnFOImIaG9vjxEjRkR3d/dx9zswMBCFQqFkAQBOXmUNlHw+HxERjY2NJesbGxuL2/L5fEyZMqVk+6hRo2LSpEnFMa+2atWqaGhoKC5eIAsAJ7eq+ByUpUuXRn9/f3HZu3dvpacEAAyhsgZKLpeLiIje3t6S9b29vcVtuVwu9u/fX7L9lVdeiQMHDhTHvFptbW3xBbFeGAsAJ7+yBkpLS0vkcrno7OwsrisUCtHd3R1tbW0REdHW1hZ9fX3R09NTHPPAAw/E4OBgtLa2lnM6AECVesNvM37xxRfj+eefLz7evXt3PPXUUzFp0qSYNm1aLF68OG666aaYPn168W3GTU1NxXf6nHHGGfHRj340Pv3pT8fGjRvjyJEjcfXVV8fll19+Qu/gAQBOfm84UJ588sn48Ic/XHy8ZMmSiIhYsGBBbN68OW644YY4dOhQLFy4MPr6+uKCCy6IrVu3Fj8DJSLiG9/4Rlx99dVx4YUXFj+obd26dWU4HADgZPCWPgelUnwOCgBUn4p9DgoAQDkIFAAgOQIFAEiOQAEAkiNQAIDkvOG3GQPAyWrW9XdVegpVq2fNJ8u6P3dQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI7PQQGoMJ+98eaV+7M3SIc7KABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQnFGVngAw/GZdf1elp1DVetZ8stJTgJOeOygAQHIECgCQHIECACRHoAAAyREoAEByyh4oR48ejWXLlkVLS0uMHTs2fv/3fz/+4R/+IbIsK47JsiyWL18eU6dOjbFjx0Z7e3s899xz5Z4KAFClyh4ot9xyS2zYsCG++tWvxs6dO+OWW26J1atXx1e+8pXimNWrV8e6deti48aN0d3dHaeeemp0dHTE4cOHyz0dAKAKlf1zUB599NG45JJLYs6cORER8Y53vCO++c1vxuOPPx4R/3/3ZO3atfH5z38+LrnkkoiIuOuuu6KxsTHuu+++uPzyy4/Z58DAQAwMDBQfFwqFck8bAEhI2e+gfOADH4jOzs742c9+FhERP/7xj+ORRx6Jiy++OCIidu/eHfl8Ptrb24s/09DQEK2trdHV1XXcfa5atSoaGhqKS3Nzc7mnDQAkpOx3UG688cYoFAoxY8aMGDlyZBw9ejS++MUvxvz58yMiIp/PR0REY2Njyc81NjYWt73a0qVLY8mSJcXHhUJBpADASazsgXLPPffEN77xjbj77rtj5syZ8dRTT8XixYujqakpFixY8Kb2WVtbG7W1tWWeKQCQqrIHyvXXXx833nhj8bUkZ599dvziF7+IVatWxYIFCyKXy0VERG9vb0ydOrX4c729vXHuueeWezoAQBUq+2tQ/vd//zdGjCjd7ciRI2NwcDAiIlpaWiKXy0VnZ2dxe6FQiO7u7mhrayv3dACAKlT2Oygf//jH44tf/GJMmzYtZs6cGTt27Ihbb701PvWpT0VERE1NTSxevDhuuummmD59erS0tMSyZcuiqakp5s6dW+7pAABVqOyB8pWvfCWWLVsWn/3sZ2P//v3R1NQUn/nMZ2L58uXFMTfccEMcOnQoFi5cGH19fXHBBRfE1q1bo66urtzTAQCqUNkDZfz48bF27dpYu3bta46pqamJlStXxsqVK8v96wGAk4Dv4gEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkDEmg/PKXv4w///M/j8mTJ8fYsWPj7LPPjieffLK4PcuyWL58eUydOjXGjh0b7e3t8dxzzw3FVACAKlT2QPnVr34V559/fowePTq+//3vx7PPPhv/9E//FBMnTiyOWb16daxbty42btwY3d3dceqpp0ZHR0ccPny43NMBAKrQqHLv8JZbbonm5ubYtGlTcV1LS0vxn7Msi7Vr18bnP//5uOSSSyIi4q677orGxsa477774vLLLz9mnwMDAzEwMFB8XCgUyj1tACAhZb+D8m//9m8xe/bs+LM/+7OYMmVKnHfeeXHHHXcUt+/evTvy+Xy0t7cX1zU0NERra2t0dXUdd5+rVq2KhoaG4tLc3FzuaQMACSl7oPz85z+PDRs2xPTp0+MHP/hB/M3f/E387d/+bdx5550REZHP5yMiorGxseTnGhsbi9tebenSpdHf319c9u7dW+5pAwAJKfufeAYHB2P27NnxpS99KSIizjvvvHjmmWdi48aNsWDBgje1z9ra2qitrS3nNAGAhJX9DsrUqVPjzDPPLFl3xhlnxJ49eyIiIpfLRUREb29vyZje3t7iNgDgd1vZA+X888+PXbt2laz72c9+FqeffnpE/P8LZnO5XHR2dha3FwqF6O7ujra2tnJPBwCoQmX/E8+1114bH/jAB+JLX/pSfOITn4jHH388br/99rj99tsjIqKmpiYWL14cN910U0yfPj1aWlpi2bJl0dTUFHPnzi33dACAKlT2QHnf+94X9957byxdujRWrlwZLS0tsXbt2pg/f35xzA033BCHDh2KhQsXRl9fX1xwwQWxdevWqKurK/d0AIAqVPZAiYj42Mc+Fh/72Mdec3tNTU2sXLkyVq5cORS/HgCockMSKCmZdf1dlZ5C1epZ88my7s+1eGvKfT0AUubLAgGA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5Ax5oNx8881RU1MTixcvLq47fPhwLFq0KCZPnhzjxo2LefPmRW9v71BPBQCoEkMaKE888UR87Wtfi/e85z0l66+99tq4//77Y8uWLbF9+/bYt29fXHrppUM5FQCgigxZoLz44osxf/78uOOOO2LixInF9f39/fH1r389br311vjIRz4Ss2bNik2bNsWjjz4ajz322HH3NTAwEIVCoWQBAE5eQxYoixYtijlz5kR7e3vJ+p6enjhy5EjJ+hkzZsS0adOiq6vruPtatWpVNDQ0FJfm5uahmjYAkIAhCZRvfetb8aMf/ShWrVp1zLZ8Ph9jxoyJCRMmlKxvbGyMfD5/3P0tXbo0+vv7i8vevXuHYtoAQCJGlXuHe/fujb/7u7+Lbdu2RV1dXVn2WVtbG7W1tWXZFwCQvrLfQenp6Yn9+/fHe9/73hg1alSMGjUqtm/fHuvWrYtRo0ZFY2NjvPzyy9HX11fyc729vZHL5co9HQCgCpX9DsqFF14YTz/9dMm6K6+8MmbMmBGf+9znorm5OUaPHh2dnZ0xb968iIjYtWtX7NmzJ9ra2so9HQCgCpU9UMaPHx9nnXVWybpTTz01Jk+eXFx/1VVXxZIlS2LSpElRX18f11xzTbS1tcX73//+ck8HAKhCZQ+UE/HlL385RowYEfPmzYuBgYHo6OiI2267rRJTAQASNCyB8tBDD5U8rquri/Xr18f69euH49cDAFXGd/EAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcgQKAJAcgQIAJEegAADJESgAQHIECgCQHIECACRHoAAAyREoAEByBAoAkByBAgAkR6AAAMkRKABAcsoeKKtWrYr3ve99MX78+JgyZUrMnTs3du3aVTLm8OHDsWjRopg8eXKMGzcu5s2bF729veWeCgBQpcoeKNu3b49FixbFY489Ftu2bYsjR47ERRddFIcOHSqOufbaa+P++++PLVu2xPbt22Pfvn1x6aWXlnsqAECVGlXuHW7durXk8ebNm2PKlCnR09MTH/rQh6K/vz++/vWvx9133x0f+chHIiJi06ZNccYZZ8Rjjz0W73//+4/Z58DAQAwMDBQfFwqFck8bAEjIkL8Gpb+/PyIiJk2aFBERPT09ceTIkWhvby+OmTFjRkybNi26urqOu49Vq1ZFQ0NDcWlubh7qaQMAFTSkgTI4OBiLFy+O888/P84666yIiMjn8zFmzJiYMGFCydjGxsbI5/PH3c/SpUujv7+/uOzdu3copw0AVFjZ/8TzmxYtWhTPPPNMPPLII29pP7W1tVFbW1umWQEAqRuyOyhXX311fOc734kHH3ww3v72txfX53K5ePnll6Ovr69kfG9vb+RyuaGaDgBQRcoeKFmWxdVXXx333ntvPPDAA9HS0lKyfdasWTF69Ojo7Owsrtu1a1fs2bMn2trayj0dAKAKlf1PPIsWLYq77747vv3tb8f48eOLrytpaGiIsWPHRkNDQ1x11VWxZMmSmDRpUtTX18c111wTbW1tx30HDwDwu6fsgbJhw4aIiPijP/qjkvWbNm2Kv/zLv4yIiC9/+csxYsSImDdvXgwMDERHR0fcdttt5Z4KAFClyh4oWZa97pi6urpYv359rF+/vty/HgA4CfguHgAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDkCBQBIjkABAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEhORQNl/fr18Y53vCPq6uqitbU1Hn/88UpOBwBIRMUC5V//9V9jyZIlsWLFivjRj34U55xzTnR0dMT+/fsrNSUAIBGjKvWLb7311vj0pz8dV155ZUREbNy4Mb773e/Gv/zLv8SNN95YMnZgYCAGBgaKj/v7+yMiolAovO7vOTrwUhln/bvlRM7vG+FavDXlvB6uxVvjuZEO1yIdJ3Itfj0my7LX32FWAQMDA9nIkSOze++9t2T9Jz/5yexP/uRPjhm/YsWKLCIsFovFYrGcBMvevXtftxUqcgflv//7v+Po0aPR2NhYsr6xsTH+/d///ZjxS5cujSVLlhQfDw4OxoEDB2Ly5MlRU1Mz5PMdKoVCIZqbm2Pv3r1RX19f6en8TnMt0uFapMO1SMfJci2yLIuDBw9GU1PT646t2J943oja2tqora0tWTdhwoTKTGYI1NfXV/W/cCcT1yIdrkU6XIt0nAzXoqGh4YTGVeRFsqeddlqMHDkyent7S9b39vZGLperxJQAgIRUJFDGjBkTs2bNis7OzuK6wcHB6OzsjLa2tkpMCQBISMX+xLNkyZJYsGBBzJ49O/7wD/8w1q5dG4cOHSq+q+d3QW1tbaxYseKYP18x/FyLdLgW6XAt0vG7eC1qsuxE3uszNL761a/GmjVrIp/Px7nnnhvr1q2L1tbWSk0HAEhERQMFAOB4fBcPAJAcgQIAJEegAADJESgAQHIEyhB7+OGH4+Mf/3g0NTVFTU1N3HfffSXbsyyL5cuXx9SpU2Ps2LHR3t4ezz33XGUme5Ipx7k/cOBAzJ8/P+rr62PChAlx1VVXxYsvvjiMR1Gdhuvc/+QnP4kPfvCDUVdXF83NzbF69eqhPrSq84UvfCFqampKlhkzZhS3Hz58OBYtWhSTJ0+OcePGxbx58475EM09e/bEnDlz4pRTTokpU6bE9ddfH6+88krJmIceeije+973Rm1tbfzBH/xBbN68eTgOL2kpPQ+2bNkSM2bMiLq6ujj77LPje9/7XtmPt9wEyhA7dOhQnHPOObF+/frjbl+9enWsW7cuNm7cGN3d3XHqqadGR0dHHD58eJhnevIpx7mfP39+/PSnP41t27bFd77znXj44Ydj4cKFw3UIVWs4zn2hUIiLLrooTj/99Ojp6Yk1a9bEF77whbj99tuH/PiqzcyZM+OFF14oLo888khx27XXXhv3339/bNmyJbZv3x779u2LSy+9tLj96NGjMWfOnHj55Zfj0UcfjTvvvDM2b94cy5cvL47ZvXt3zJkzJz784Q/HU089FYsXL46/+qu/ih/84AfDepypSeV58Oijj8YVV1wRV111VezYsSPmzp0bc+fOjWeeeWboDr4c3uIXE/MGRETJNzgPDg5muVwuW7NmTXFdX19fVltbm33zm9+swAxPXm/m3D/77LNZRGRPPPFEccz3v//9rKamJvvlL385bHOvdkN17m+77bZs4sSJ2cDAQHHM5z73uezd7373EB9RdVmxYkV2zjnnHHdbX19fNnr06GzLli3FdTt37swiIuvq6sqyLMu+973vZSNGjMjy+XxxzIYNG7L6+vriub/hhhuymTNnluz7sssuyzo6Osp8NNWrks+DT3ziE9mcOXNK5tPa2pp95jOfKesxlps7KBW0e/fuyOfz0d7eXlzX0NAQra2t0dXVVcGZnfxO5Nx3dXXFhAkTYvbs2cUx7e3tMWLEiOju7h72OZ8synXuu7q64kMf+lCMGTOmOKajoyN27doVv/rVr4bpaKrDc889F01NTfHOd74z5s+fH3v27ImIiJ6enjhy5EjJtZgxY0ZMmzat5FqcffbZJd8+39HREYVCIX76058Wx/zmPn49xn/HXttwPg+q9foIlArK5/MRESVP/F8//vU2hsaJnPt8Ph9Tpkwp2T5q1KiYNGmS6/MWlOvc5/P54+7jN38HEa2trbF58+bYunVrbNiwIXbv3h0f/OAH4+DBg5HP52PMmDHHfDv8q6/F653n1xpTKBTipZdeGqIjq27D+Tx4rTGpP08q9l08AAy9iy++uPjP73nPe6K1tTVOP/30uOeee2Ls2LEVnBn8du6gVFAul4uIOOYV8729vcVtDI0TOfe5XC72799fsv2VV16JAwcOuD5vQbnOfS6XO+4+fvN3cKwJEybEu971rnj++ecjl8vFyy+/HH19fSVjXn0tXu88v9aY+vp6EfQahvN58FpjUn+eCJQKamlpiVwuF52dncV1hUIhuru7o62trYIzO/mdyLlva2uLvr6+6OnpKY554IEHYnBw0JdavgXlOvdtbW3x8MMPx5EjR4pjtm3bFu9+97tj4sSJw3Q01efFF1+M//iP/4ipU6fGrFmzYvTo0SXXYteuXbFnz56Sa/H000+X/I9y27ZtUV9fH2eeeWZxzG/u49dj/HfstQ3n86Bqr0+lX6V7sjt48GC2Y8eObMeOHVlEZLfeemu2Y8eO7Be/+EWWZVl28803ZxMmTMi+/e1vZz/5yU+ySy65JGtpacleeumlCs+8+pXj3H/0ox/NzjvvvKy7uzt75JFHsunTp2dXXHFFpQ6pagzHue/r68saGxuzv/iLv8ieeeaZ7Fvf+lZ2yimnZF/72teG/XhTdt1112UPPfRQtnv37uyHP/xh1t7enp122mnZ/v37syzLsr/+67/Opk2blj3wwAPZk08+mbW1tWVtbW3Fn3/llVeys846K7vooouyp556Ktu6dWv2e7/3e9nSpUuLY37+859np5xySnb99ddnO3fuzNavX5+NHDky27p167Afb0pSeR788Ic/zEaNGpX94z/+Y7Zz585sxYoV2ejRo7Onn356+E7GmyBQhtiDDz6YRcQxy4IFC7Is+/+3mi1btixrbGzMamtrswsvvDDbtWtXZSd9kijHuf+f//mf7IorrsjGjRuX1dfXZ1deeWV28ODBChxNdRmuc//jH/84u+CCC7La2trsbW97W3bzzTcP1yFWjcsuuyybOnVqNmbMmOxtb3tbdtlll2XPP/98cftLL72Uffazn80mTpyYnXLKKdmf/umfZi+88ELJPv7zP/8zu/jii7OxY8dmp512WnbddddlR44cKRnz4IMPZueee242ZsyY7J3vfGe2adOm4Ti8pKX0PLjnnnuyd73rXdmYMWOymTNnZt/97neH7LjLpSbLsmz47tcAALw+r0EBAJIjUACA5AgUACA5AgUASI5AAQCSI1AAgOQIFAAgOQIFAEiOQAEAkiNQAIDkCBQAIDn/B9yiKNmTNLn8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_combined = [accuracy_10, accuracy_100, accuracy_1000, accuracy_5000, accuracy_10000]\n",
    "\n",
    "sns.barplot(x=epochs, y=accuracy_combined)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "As seen in the above graph, after training the model with multiple epoch counts, the accuracy increases as the epoch count increased and the jump was visibly higher between 10, 100 and 1000. The increase in accuracy between 1000, 5000 and 10000 is not that high but still significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "The model is hard-coded and also very static as of right now. It could be made more dynamic by adding logic for adding different layers in Keras style (Ex:model.add(InputLayer())). Improvements to include different loss functions, activation functions, optimizers can also be made. The model could also be moreobject oriented by defining a base Layer class which has children InputLayer, HiddenLayer or Dense, OutputLayer etc. Neuron class with seperate weights andbias could also be implemented.\n",
    "\n",
    "Additonally, differnt model callbacks like ModelCheckpoint to save the model's weights and bias, EarlyStopping to stop the model early if the training or validationloss is not improving, ReduceLearningRateOnPlateau to decrease learning rate if no improvement in loss etc. could also be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and Acknowledgements\n",
    "\n",
    "1. https://medium.com/@waadlingaadil/learn-to-build-a-neural-network-from-scratch-yes-really-cac4ca457efc - Thanks to this article, I learned 90% of what ANNs actually do.\n",
    "2. https://medium.com/analytics-vidhya/creating-keras-from-scratch-part-1-50599413ebc7\n",
    "3. https://www.parasdahal.com/softmax-crossentropy\n",
    "4. https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=tMVHHor1fqCwSSF7 - A must watch for learning Neural Networks\n",
    "5. https://youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&si=7GLSl2fbrpdAynCE - Another golden playlist to understand the concepts of Neural Networks\n",
    "\n",
    "Finally, my sincere thanks to Dr. Huajie Zhang for providing such a wonderful assignment. I learned many things in the process learning and implementing a neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
